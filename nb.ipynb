{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medmnist.dataset import (PathMNIST, ChestMNIST, DermaMNIST, OCTMNIST, PneumoniaMNIST, RetinaMNIST,\n",
    "                                  BreastMNIST, BloodMNIST, TissueMNIST, OrganAMNIST, OrganCMNIST, OrganSMNIST,\n",
    "                                  OrganMNIST3D, NoduleMNIST3D, AdrenalMNIST3D, FractureMNIST3D, VesselMNIST3D, SynapseMNIST3D)\n",
    "from medmnist.dataset import MedMNIST\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from typing import Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = (PathMNIST, ChestMNIST, DermaMNIST, OCTMNIST, PneumoniaMNIST, RetinaMNIST,\n",
    "                                  BreastMNIST, BloodMNIST, TissueMNIST, OrganAMNIST, OrganCMNIST, OrganSMNIST,\n",
    "                                  OrganMNIST3D, NoduleMNIST3D, AdrenalMNIST3D, FractureMNIST3D, VesselMNIST3D, SynapseMNIST3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights, resnet34, resnet50, resnet101, resnet152, ResNet34_Weights, ResNet50_Weights, ResNet101_Weights, ResNet152_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "class StratifiedSplitter:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def split(self, strat_array, n_splits):\n",
    "        self.n_splits = n_splits\n",
    "        self.strat_array = strat_array\n",
    "        self.masks = {i: np.zeros(len(strat_array)).astype(bool) for i in range(n_splits)}\n",
    "        skf = StratifiedKFold(n_splits=self.n_splits, random_state=42, shuffle=True)\n",
    "        for i, (train_index, test_index) in enumerate(skf.split(self.strat_array, self.strat_array)):\n",
    "            self.masks[i][train_index] = True\n",
    "\n",
    "    def get_train(self, fold):\n",
    "        return self.masks[fold]\n",
    "\n",
    "    def get_test(self, fold):\n",
    "        return ~self.masks[fold]\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        with open(path, 'rb') as file:\n",
    "            loaded = pickle.load(file)\n",
    "        cl = StratifiedSplitter()\n",
    "        cl.masks = loaded\n",
    "        # Note: n_splits and strat_array aren't saved/loaded, might be needed if reusing methods other than get_train/test\n",
    "        return cl\n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, 'wb') as file:\n",
    "            pickle.dump(self.masks, file)\n",
    "    \n",
    "class ResnetModel(nn.Module):\n",
    "  def __init__(self, n_classes, size=18, activation='sigmoid'):\n",
    "    super(ResnetModel, self).__init__()\n",
    "    self.backbone = self._backbone(size)\n",
    "    self.fc = nn.Linear(self.backbone.fc.in_features, n_classes)\n",
    "    self.backbone.fc = nn.Identity()\n",
    "    if activation == 'sigmoid':\n",
    "      self.activation = nn.Sigmoid()\n",
    "    elif activation == 'softmax':\n",
    "      self.activation = nn.LogSoftmax(dim=1)\n",
    "    else:\n",
    "      raise ValueError(f\"Activation {activation} is not supported\")\n",
    "\n",
    "  def _backbone(self, size):\n",
    "    if size == 18:\n",
    "      backbone = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    elif size == 34:\n",
    "      backbone = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "    elif size == 50:\n",
    "      backbone = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    elif size == 101:\n",
    "      backbone = resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "    elif size == 152:\n",
    "      backbone = resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "    else:\n",
    "      raise ValueError(\"Invalid ResNet size. Choose from 18, 34, 50, 101, or 152.\")\n",
    "    return backbone\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.backbone(x)\n",
    "    x = self.fc(x)\n",
    "    x = self.activation(x)\n",
    "    return x\n",
    "\n",
    "class Medmnist2DModel(pl.LightningModule):\n",
    "  def __init__(self, model, loss):\n",
    "    super(Medmnist2DModel, self).__init__()\n",
    "    self.loss = loss\n",
    "    self.model = model\n",
    "  \n",
    "    self.learning_rate = 3e-4\n",
    "    self.wd = 1e-4\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y_hat = self.model(x)\n",
    "    loss = self.loss(y_hat, y.float())\n",
    "    self.log(\"train_loss\", loss, on_epoch=True, logger=True, on_step=True)\n",
    "    return loss\n",
    " \n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y_hat = self.model(x)\n",
    "    loss = self.loss(y_hat, y.float())\n",
    "    self.log(\"validation_loss\", loss, on_epoch=True, logger=True, on_step=True)\n",
    "    return loss\n",
    "\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "      opt = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.wd)\n",
    "      out = opt\n",
    "      # if self.onecycle:\n",
    "      #     scheduler = OneCycleLR(opt, max_lr=self.learning_rate, total_steps=self.trainer.estimated_stepping_batches)\n",
    "      #     lr_scheduler = {\"scheduler\": scheduler, \"interval\": \"step\"}\n",
    "      #     out = {\"optimizer\": opt, \"lr_scheduler\": lr_scheduler}\n",
    "      # elif self.reduce_on_plateau:\n",
    "      #     scheduler = ReduceLROnPlateau(opt, \"max\", factor=0.67, patience=3, eps=0.0001)\n",
    "      #     return [opt], [{\"scheduler\": scheduler, \"interval\": \"epoch\", \"monitor\": \"val_averageprecision_epoch\"}]\n",
    "      # else:\n",
    "      #     out = opt\n",
    "\n",
    "      return out\n",
    "\n",
    "class MedmnistDataModule(pl.LightningDataModule):\n",
    "  def __init__(self, dataset_type: Type[MedMNIST], train_splt='train', batch_size=128, num_workers=16, image_size=224):\n",
    "    super().__init__()\n",
    "    \n",
    "    if isinstance(train_splt, int):\n",
    "      pass # stratified splitting\n",
    "      self.splitter = 123\n",
    "    else:\n",
    "      self.splitter = None\n",
    "    self.dataset_type = dataset_type \n",
    "    self.batch_size = batch_size\n",
    "    self.num_workers = num_workers\n",
    "    self.image_size = image_size\n",
    "\n",
    "  def prepare_data(self):\n",
    "    pass \n",
    "\n",
    "  def setup(self, stage=None): # add test/predict stages that load only one dataset\n",
    "    if self.splitter is not None:\n",
    "      pass\n",
    "    else:\n",
    "      print(\"Setting up train dataset\")\n",
    "      self.train_dataset = self.dataset_type(split='train', transform=self.train_trainsforms, as_rgb=True, size=self.image_size)\n",
    "      print(\"Setting up val dataset\")\n",
    "      self.val_dataset = self.dataset_type(split='val', transform=self.val_test_transforms, as_rgb=True, size=self.image_size)\n",
    "      print(\"Setting up test dataset\")\n",
    "      self.test_dataset = self.dataset_type(split='test', transform=self.val_test_transforms, as_rgb=True, size=self.image_size)\n",
    "    return \n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(self.train_dataset, batch_size = self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "  \n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(self.val_dataset, batch_size = self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "  \n",
    "  def test_dataloader(self):\n",
    "    return DataLoader(self.test_dataset, batch_size = self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "   \n",
    "  @property\n",
    "  def train_trainsforms(self):\n",
    "    data_transform = transforms.Compose(\n",
    "      [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[.5], std=[.5])\n",
    "      ]\n",
    "    )\n",
    "    return data_transform\n",
    "\n",
    "  @property\n",
    "  def val_test_transforms(self):\n",
    "    data_transform = transforms.Compose(\n",
    "      [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[.5], std=[.5])\n",
    "      ]\n",
    "    )\n",
    "    return data_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = MedmnistDataModule(PathMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Medmnist2DModel(ResnetModel(9, 18), F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up train dataset\n",
      "Setting up val dataset\n",
      "Setting up test dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type        | Params | Mode\n",
      "---------------------------------------------\n",
      "0 | model | ResnetModel | 11.2 M | eval\n",
      "---------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.725    Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "71        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:SEND Error: Host unreachable\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3, 224, 224]), torch.Size([64, 14]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definitions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Definitions\n",
    "\n",
    "# MedMNIST Imports\n",
    "from medmnist.dataset import (PathMNIST, ChestMNIST, DermaMNIST, OCTMNIST, PneumoniaMNIST, RetinaMNIST,\n",
    "                              BreastMNIST, BloodMNIST, TissueMNIST, OrganAMNIST, OrganCMNIST, OrganSMNIST,\n",
    "                              OrganMNIST3D, NoduleMNIST3D, AdrenalMNIST3D, FractureMNIST3D, VesselMNIST3D, SynapseMNIST3D)\n",
    "from medmnist.dataset import MedMNIST\n",
    "from medmnist import INFO # Import INFO to get dataset details\n",
    "\n",
    "# PyTorch and PyTorch Lightning Imports\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import torchvision.transforms as transforms\n",
    "import torchmetrics # Import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Other Imports\n",
    "from pathlib import Path\n",
    "from typing import Type, Optional\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Model Imports (specific models)\n",
    "from torchvision.models import resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "\n",
    "# Configuration\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# --- StratifiedSplitter Class (remains unchanged) ---\n",
    "class StratifiedSplitter:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def split(self, strat_array, n_splits):\n",
    "    self.n_splits = n_splits\n",
    "    self.strat_array = strat_array\n",
    "    self.masks = {i: np.zeros(len(strat_array)).astype(bool) for i in range(n_splits)}\n",
    "    skf = StratifiedKFold(n_splits=self.n_splits, random_state=42, shuffle=True)\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(self.strat_array, self.strat_array)):\n",
    "      self.masks[i][train_index] = True\n",
    "\n",
    "  def get_train(self, fold):\n",
    "    return self.masks[fold]\n",
    "\n",
    "  def get_test(self, fold):\n",
    "    return ~self.masks[fold]\n",
    "\n",
    "  @staticmethod\n",
    "  def load(path):\n",
    "    with open(path, 'rb') as file:\n",
    "      loaded = pickle.load(file)\n",
    "    cl = StratifiedSplitter()\n",
    "    cl.masks = loaded\n",
    "    # Potential Bug: Need to set other attributes like n_splits?\n",
    "    return cl # Return the instance\n",
    "\n",
    "  def save(self, path):\n",
    "    # Potential Bug: Should serialize self.masks, not self directly\n",
    "    with open(path, 'wb') as file:\n",
    "      pickle.dump(self.masks, file) # Corrected serialization\n",
    "\n",
    "# --- ResnetModel Class (remains unchanged from previous version) ---\n",
    "class ResnetModel(nn.Module):\n",
    "  # Explicitly add activation type\n",
    "  def __init__(self, n_classes, size=18, activation_type='logsoftmax', pretrained=False):\n",
    "    super(ResnetModel, self).__init__()\n",
    "    self.backbone = self._backbone(size, pretrained) # Pass pretrained flag\n",
    "    self.fc = nn.Linear(self.backbone.fc.in_features, n_classes)\n",
    "    self.backbone.fc = nn.Identity() # Remove original fc\n",
    "\n",
    "    # Instantiate activation based on type\n",
    "    if activation_type == 'sigmoid':\n",
    "      self.activation = nn.Sigmoid()\n",
    "      print(\"Using Sigmoid activation in model.\")\n",
    "    elif activation_type == 'logsoftmax':\n",
    "      self.activation = nn.LogSoftmax(dim=1)\n",
    "      print(\"Using LogSoftmax activation in model.\")\n",
    "    elif activation_type == 'identity':\n",
    "        self.activation = nn.Identity()\n",
    "        print(\"Using Identity activation (outputting logits).\")\n",
    "    # Add placeholder for your custom GEV activation later\n",
    "    # elif activation_type == 'gev':\n",
    "    #   self.activation = YourGEVActivation(...)\n",
    "    else:\n",
    "      raise ValueError(f\"Activation type '{activation_type}' is not supported\")\n",
    "\n",
    "  def _backbone(self, size, pretrained=False):\n",
    "    weights = None # Default to no pretrained weights\n",
    "    if pretrained:\n",
    "        print(\"Warning: Pretrained weights requested but may not be optimal without ImageNet normalization and augmentations.\")\n",
    "        pass # Keep weights=None for now based on user request\n",
    "\n",
    "    if size == 18:\n",
    "      backbone = resnet18(weights=weights)\n",
    "    elif size == 34:\n",
    "      backbone = resnet34(weights=weights)\n",
    "    elif size == 50:\n",
    "      backbone = resnet50(weights=weights)\n",
    "    elif size == 101:\n",
    "      backbone = resnet101(weights=weights)\n",
    "    elif size == 152:\n",
    "      backbone = resnet152(weights=weights)\n",
    "    else:\n",
    "      raise ValueError(\"Invalid ResNet size. Choose from 18, 34, 50, 101, or 152.\")\n",
    "\n",
    "    # Input channel adaptation might be needed if as_rgb=False and channels != 3\n",
    "    # Since as_rgb=True is forced later, this is less critical now.\n",
    "    # Example:\n",
    "    # if n_channels != 3:\n",
    "    #    print(f\"Adapting ResNet conv1 for {n_channels} input channels.\")\n",
    "    #    backbone.conv1 = nn.Conv2d(n_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "    return backbone\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.backbone(x)\n",
    "    x = self.fc(x)\n",
    "    x = self.activation(x) # Apply activation\n",
    "    return x\n",
    "\n",
    "\n",
    "# --- Medmnist2DModel Class (remains unchanged from previous version) ---\n",
    "class Medmnist2DModel(pl.LightningModule):\n",
    "  # Added num_classes and task_type for metrics\n",
    "  def __init__(self, model: nn.Module, num_classes: int, task_type: str, loss_fn: nn.Module, learning_rate=3e-4, weight_decay=1e-4):\n",
    "    super(Medmnist2DModel, self).__init__()\n",
    "    self.save_hyperparameters(ignore=['model', 'loss_fn']) # Save LR, WD etc.\n",
    "\n",
    "    self.model = model\n",
    "    self.loss = loss_fn # Use the provided loss function\n",
    "\n",
    "    # --- Initialize Metrics ---\n",
    "    if task_type not in ['binary', 'multiclass', 'multilabel']:\n",
    "        raise ValueError(f\"Invalid task_type '{task_type}' for torchmetrics.\")\n",
    "\n",
    "    common_metric_args = {'task': task_type}\n",
    "    if task_type != 'binary':\n",
    "         common_metric_args['num_classes'] = num_classes\n",
    "\n",
    "    self.val_auc = torchmetrics.AUROC(**common_metric_args)\n",
    "    self.val_ap = torchmetrics.AveragePrecision(**common_metric_args)\n",
    "    self.test_auc = torchmetrics.AUROC(**common_metric_args)\n",
    "    self.test_ap = torchmetrics.AveragePrecision(**common_metric_args)\n",
    "\n",
    "    self.validation_step_outputs = []\n",
    "    self.test_step_outputs = []\n",
    "\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y_hat = self.model(x)\n",
    "\n",
    "    if isinstance(self.loss, (nn.NLLLoss, nn.CrossEntropyLoss)):\n",
    "        y = y.squeeze().long()\n",
    "    elif isinstance(self.loss, (nn.BCEWithLogitsLoss, nn.BCELoss)):\n",
    "         y = y.float()\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    loss = self.loss(y_hat, y)\n",
    "    self.log(\"train_loss\", loss, on_epoch=True, logger=True, on_step=True, prog_bar=True)\n",
    "    return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y_hat_activated = self.model(x)\n",
    "\n",
    "    if isinstance(self.loss, (nn.NLLLoss, nn.CrossEntropyLoss)):\n",
    "        y_true_loss = y.squeeze().long()\n",
    "    elif isinstance(self.loss, (nn.BCEWithLogitsLoss, nn.BCELoss)):\n",
    "         y_true_loss = y.float()\n",
    "    else:\n",
    "        y_true_loss = y\n",
    "\n",
    "    loss = self.loss(y_hat_activated, y_true_loss)\n",
    "    self.log(\"val_loss\", loss, on_epoch=True, logger=True, on_step=False)\n",
    "\n",
    "    y_true_metric = y.squeeze().int()\n",
    "\n",
    "    if isinstance(self.loss, nn.NLLLoss):\n",
    "        y_hat_probs = torch.exp(y_hat_activated)\n",
    "    elif isinstance(self.loss, nn.CrossEntropyLoss):\n",
    "        y_hat_probs = torch.softmax(y_hat_activated, dim=1)\n",
    "    elif isinstance(self.loss, nn.BCEWithLogitsLoss):\n",
    "         y_hat_probs = torch.sigmoid(y_hat_activated)\n",
    "    elif isinstance(self.loss, nn.BCELoss):\n",
    "         y_hat_probs = y_hat_activated\n",
    "    else:\n",
    "        y_hat_probs = y_hat_activated\n",
    "\n",
    "    self.val_auc.update(y_hat_probs, y_true_metric)\n",
    "    self.val_ap.update(y_hat_probs, y_true_metric)\n",
    "\n",
    "    self.validation_step_outputs.append(loss)\n",
    "    return loss\n",
    "\n",
    "  def on_validation_epoch_end(self):\n",
    "    auc = self.val_auc.compute()\n",
    "    ap = self.val_ap.compute()\n",
    "    self.log('val_auc_epoch', auc, prog_bar=True)\n",
    "    self.log('val_ap_epoch', ap, prog_bar=True)\n",
    "    self.val_auc.reset()\n",
    "    self.val_ap.reset()\n",
    "    self.validation_step_outputs.clear()\n",
    "\n",
    "  # Add test_step and on_test_epoch_end similarly if needed\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "      opt = torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate, weight_decay=self.hparams.weight_decay)\n",
    "      return opt\n",
    "\n",
    "\n",
    "# --- MedmnistDataModule modified to load native size ---\n",
    "class MedmnistDataModule(pl.LightningDataModule):\n",
    "  # Removed data_root from __init__\n",
    "  def __init__(self, dataset_type: Type[MedMNIST],\n",
    "               image_size: int, # Specify the desired MedMNIST native size\n",
    "               train_splt='train',\n",
    "               batch_size=128,\n",
    "               num_workers=16,\n",
    "               as_rgb=True,\n",
    "               download=True, # Keep download flag\n",
    "               splitter=None):\n",
    "    super().__init__()\n",
    "\n",
    "    if image_size not in dataset_type.available_sizes:\n",
    "        raise ValueError(f\"Size {image_size} not available for {dataset_type.flag}. Available sizes: {dataset_type.available_sizes}\")\n",
    "\n",
    "    self.dataset_type = dataset_type\n",
    "    self.image_size_to_load = image_size\n",
    "    self.train_splt_config = train_splt\n",
    "    self.batch_size = batch_size\n",
    "    self.num_workers = num_workers\n",
    "    self.as_rgb = as_rgb\n",
    "    self.download = download # Still useful to control download behavior\n",
    "    self.splitter = splitter\n",
    "    # self.data_root is no longer needed here\n",
    "\n",
    "    if not self.as_rgb:\n",
    "        print(\"Warning: as_rgb=False. Standard ResNet expects 3 channels...\")\n",
    "\n",
    "\n",
    "  def prepare_data(self):\n",
    "    # Download data for the specified size using the default root\n",
    "    if self.download:\n",
    "        print(f\"Downloading {self.dataset_type.flag} size {self.image_size_to_load} (using default root ~/.medmnist)...\")\n",
    "        # Omit the root argument here\n",
    "        _ = self.dataset_type(split='train', download=True, size=self.image_size_to_load)\n",
    "\n",
    "\n",
    "  def setup(self, stage: Optional[str] = None):\n",
    "    # Omit the root argument when creating datasets\n",
    "    common_args = {\n",
    "        'as_rgb': self.as_rgb,\n",
    "        'size': self.image_size_to_load,\n",
    "        'download': False # Already handled in prepare_data\n",
    "        # root argument omitted here\n",
    "    }\n",
    "\n",
    "    if stage == 'fit' or stage is None:\n",
    "        print(f\"Setting up train/val datasets for {self.dataset_type.flag} (size {self.image_size_to_load})...\")\n",
    "        if self.splitter is not None and isinstance(self.train_splt_config, int):\n",
    "             raise NotImplementedError(\"Stratified splitting not fully implemented yet.\")\n",
    "        else:\n",
    "             print(f\"Using predefined 'train' and 'val' splits.\")\n",
    "             self.train_dataset = self.dataset_type(split='train', transform=self.train_transforms, **common_args)\n",
    "             self.val_dataset = self.dataset_type(split='val', transform=self.val_test_transforms, **common_args)\n",
    "\n",
    "    if stage == 'test' or stage is None:\n",
    "        print(f\"Setting up test dataset for {self.dataset_type.flag} (size {self.image_size_to_load})...\")\n",
    "        self.test_dataset = self.dataset_type(split='test', transform=self.val_test_transforms, **common_args)\n",
    "\n",
    "    print(\"Dataset setup finished.\")\n",
    "\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(self.train_dataset, batch_size = self.batch_size, shuffle=True, num_workers=self.num_workers, persistent_workers=True if self.num_workers > 0 else False, pin_memory=True)\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(self.val_dataset, batch_size = self.batch_size, shuffle=False, num_workers=self.num_workers, persistent_workers=True if self.num_workers > 0 else False, pin_memory=True)\n",
    "\n",
    "  def test_dataloader(self):\n",
    "    return DataLoader(self.test_dataset, batch_size = self.batch_size, shuffle=False, num_workers=self.num_workers, persistent_workers=True if self.num_workers > 0 else False, pin_memory=True)\n",
    "\n",
    "  @property\n",
    "  def train_transforms(self):\n",
    "    trans = []\n",
    "    # --- No Resize needed as we load the correct size ---\n",
    "    # --- No Augmentation Added Here (as requested) ---\n",
    "    trans.append(transforms.ToTensor())\n",
    "    # Normalize for 3 channels since as_rgb=True\n",
    "    trans.append(transforms.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5]))\n",
    "    return transforms.Compose(trans)\n",
    "\n",
    "  @property\n",
    "  def val_test_transforms(self):\n",
    "    trans = []\n",
    "    # --- No Resize needed ---\n",
    "    trans.append(transforms.ToTensor())\n",
    "    # Normalize for 3 channels\n",
    "    trans.append(transforms.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5]))\n",
    "    return transforms.Compose(trans)\n",
    "\n",
    "print(\"Definitions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Dataset: pathmnist\n",
      "Task: multi-class, Metric Task: multiclass, Num Classes: 9\n",
      "Model Size: 18, Activation: logsoftmax, Loss: NLLLoss\n",
      "Image Load Size: 224\n",
      "Using LogSoftmax activation in model.\n",
      "Starting training for 10 epochs...\n",
      "Downloading pathmnist size 224 (using default root ~/.medmnist)...\n",
      "Setting up train/val datasets for pathmnist (size 224)...\n",
      "Using predefined 'train' and 'val' splits.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset setup finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type                       | Params | Mode \n",
      "----------------------------------------------------------------\n",
      "0 | model    | ResnetModel                | 11.2 M | train\n",
      "1 | loss     | NLLLoss                    | 0      | train\n",
      "2 | val_auc  | MulticlassAUROC            | 0      | train\n",
      "3 | val_ap   | MulticlassAveragePrecision | 0      | train\n",
      "4 | test_auc | MulticlassAUROC            | 0      | train\n",
      "5 | test_ap  | MulticlassAveragePrecision | 0      | train\n",
      "----------------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.725    Total estimated model params size (MB)\n",
      "76        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1407/1407 [00:45<00:00, 30.76it/s, v_num=3, train_loss_step=0.0463, val_auc_epoch=1.000, val_ap_epoch=0.999, train_loss_epoch=0.0335]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1407/1407 [00:45<00:00, 30.61it/s, v_num=3, train_loss_step=0.0463, val_auc_epoch=1.000, val_ap_epoch=0.999, train_loss_epoch=0.0335]\n",
      "Run finished. Check TensorBoard logs in ./lightning_logs\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration and Execution\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_CLASS = PathMNIST\n",
    "IMAGE_SIZE = 224\n",
    "MODEL_SIZE = 18\n",
    "ACTIVATION = 'logsoftmax'\n",
    "LOSS_FN = nn.NLLLoss()\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# --- Get Dataset Info ---\n",
    "info = INFO[DATASET_CLASS.flag]\n",
    "n_classes = len(info['label'])\n",
    "task = info['task']\n",
    "\n",
    "# Map MedMNIST task to torchmetrics task type\n",
    "if task == 'multi-class' or task == 'ordinal-regression':\n",
    "    metric_task_type = 'multiclass'\n",
    "elif task == 'binary-class':\n",
    "    metric_task_type = 'binary'\n",
    "elif task == 'multi-label, binary-class':\n",
    "    metric_task_type = 'multilabel'\n",
    "    if isinstance(LOSS_FN, nn.NLLLoss):\n",
    "         print(\"\\n*** WARNING: NLLLoss is not typical for multi-label tasks. Consider BCEWithLogitsLoss and model activation='identity'. ***\\n\")\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported task type: {task}\")\n",
    "\n",
    "\n",
    "# --- Initialize Modules ---\n",
    "print(f\"Using Dataset: {DATASET_CLASS.flag}\")\n",
    "print(f\"Task: {task}, Metric Task: {metric_task_type}, Num Classes: {n_classes}\")\n",
    "print(f\"Model Size: {MODEL_SIZE}, Activation: {ACTIVATION}, Loss: {type(LOSS_FN).__name__}\")\n",
    "print(f\"Image Load Size: {IMAGE_SIZE}\") # Changed from Resize Target\n",
    "\n",
    "# Check if the chosen size is available for the dataset before instantiating DataModule\n",
    "if IMAGE_SIZE not in DATASET_CLASS.available_sizes:\n",
    "     raise ValueError(f\"Size {IMAGE_SIZE} is not available for {DATASET_CLASS.flag}. Available: {DATASET_CLASS.available_sizes}\")\n",
    "\n",
    "\n",
    "dm = MedmnistDataModule(\n",
    "    dataset_type=DATASET_CLASS,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    as_rgb=True,\n",
    "    download=True # Control download still useful\n",
    ")\n",
    "\n",
    "# Instantiate the model (no pretraining)\n",
    "model_core = ResnetModel(n_classes=n_classes, size=MODEL_SIZE, activation_type=ACTIVATION, pretrained=False)\n",
    "\n",
    "# Instantiate the LightningModule\n",
    "lightning_model = Medmnist2DModel(\n",
    "    model=model_core,\n",
    "    num_classes=n_classes,\n",
    "    task_type=metric_task_type,\n",
    "    loss_fn=LOSS_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# --- Initialize Trainer ---\n",
    "# Using default TensorBoardLogger. Logs stored in ./lightning_logs/\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    logger=True, # TensorBoardLogger logs to ./lightning_logs\n",
    "    callbacks=[pl.callbacks.TQDMProgressBar(refresh_rate=10)],\n",
    "    # Add other callbacks like ModelCheckpoint if needed\n",
    "    # callbacks=[pl.callbacks.ModelCheckpoint(monitor='val_auc_epoch', mode='max')]\n",
    ")\n",
    "\n",
    "# --- Run Training ---\n",
    "print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "trainer.fit(lightning_model, dm)\n",
    "\n",
    "# --- Optional: Run Testing ---\n",
    "# print(\"Starting testing...\")\n",
    "# trainer.test(lightning_model, datamodule=dm)\n",
    "\n",
    "print(f\"Run finished. Check TensorBoard logs in ./lightning_logs\")\n",
    "\n",
    "# To view logs: tensorboard --logdir ./lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medmnist import ChestMNIST\n",
    "dataset = ChestMNIST(split=\"train\", download=False, size=224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    42405\n",
       "1    21602\n",
       "2     9970\n",
       "3     3378\n",
       "4      829\n",
       "5      218\n",
       "6       49\n",
       "7       14\n",
       "9        2\n",
       "8        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.Series(dataset.labels.sum(axis=1)).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = dataset.labels\n",
    "stratify_targets = np.array([''.join(map(str, row)) for row in targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00000000000000', '00000000000000', '00000000000000', ...,\n",
       "       '00010100000010', '00000000000000', '00000000000000'],\n",
       "      shape=(78468,), dtype='<U14')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stratify_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.tensor([0,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=224x224>, array([0]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Subset' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlabels\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Subset' object has no attribute 'labels'"
     ]
    }
   ],
   "source": [
    "sb.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s/Git/MedMNIST/.venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "splitter = StratifiedSplitter()\n",
    "splitter.split(strat_array=stratify_targets, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False,  True, False], shape=(78468,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter.get_test(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
